{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a18dca",
   "metadata": {},
   "source": [
    "##Aaryan Thapa(06)\n",
    "##Aaditya Sapkota(01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c6359",
   "metadata": {},
   "source": [
    "# Bitcoin Price Prediction Using Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "The objective of this project is to predict the daily closing price of Bitcoin using historical market data.\n",
    "\n",
    "Bitcoin is a highly volatile digital asset, and understanding how different market indicators influence its closing price is important for financial analysis and decision-making.\n",
    "\n",
    "This is a regression problem because the target variable (Close price) is a continuous numerical value.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Predict the Close price of Bitcoin.\n",
    "- Use historical features such as:\n",
    "  - Open price\n",
    "  - High price\n",
    "  - Low price\n",
    "  - Trading Volume\n",
    "  - 7-day Moving Average (MA_7)\n",
    "  - 30-day Moving Average (MA_30)\n",
    "- Train a model from scratch without using external machine learning libraries.\n",
    "- Evaluate model performance using MSE, RMSE, and R² score.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Linear Regression?\n",
    "\n",
    "Linear Regression was selected because:\n",
    "\n",
    "- The target variable is continuous.\n",
    "- Financial price indicators often have a linear relationship.\n",
    "- It is simple, interpretable, and mathematically strong.\n",
    "- It can be implemented from scratch using basic mathematics.\n",
    "- It allows us to understand how each feature influences the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The following steps were followed in this project:\n",
    "\n",
    "1. Load the dataset using the CSV module.\n",
    "2. Clean the data by removing missing or invalid values.\n",
    "3. Split the dataset into 80% training and 20% testing sets.\n",
    "4. Separate features (X) and target variable (y).\n",
    "5. Normalize the features using Min-Max scaling.\n",
    "6. Implement Linear Regression using Gradient Descent.\n",
    "7. Train the model for multiple epochs.\n",
    "8. Evaluate the model using performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Formula\n",
    "\n",
    "The Linear Regression model follows this equation:\n",
    "\n",
    "Close = w₁(Open) + w₂(High) + w₃(Low) + w₄(Volume) + w₅(MA₇) + w₆(MA₃₀) + b\n",
    "\n",
    "Where:\n",
    "\n",
    "- w₁ to w₆ are weights learned during training.\n",
    "- b is the bias term.\n",
    "- Gradient Descent is used to minimize the Mean Squared Error (MSE).\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "The model performance is measured using:\n",
    "\n",
    "- Mean Squared Error (MSE)  \n",
    "  Average squared difference between predicted and actual values.\n",
    "\n",
    "- Root Mean Squared Error (RMSE)  \n",
    "  Square root of MSE. Represents error in dollar value.\n",
    "\n",
    "- R² Score  \n",
    "  Measures how well the model explains variation in Bitcoin prices.  \n",
    "  R² = 1 indicates perfect prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "After training and testing:\n",
    "\n",
    "- The model produces predictions for unseen test data.\n",
    "- Performance metrics indicate how accurate the predictions are.\n",
    "- Sample predictions are compared with actual closing prices.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates how Linear Regression can be implemented from scratch to solve a real-world financial prediction problem.\n",
    "\n",
    "The model successfully learns the relationship between market indicators and Bitcoin closing price. Although financial markets are complex and not perfectly linear, the model provides a strong baseline for prediction.\n",
    "\n",
    "Future improvements could include:\n",
    "- Adding more technical indicators\n",
    "- Using polynomial regression\n",
    "- Implementing regularization\n",
    "- Trying more advanced models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0349540",
   "metadata": {},
   "source": [
    "#Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd19c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded: 3393\n",
      "Sample row: {'Date': '2014-09-27', 'Open': '403.56', 'High': '406.62', 'Low': '397.37', 'Close': '399.52', 'Volume': '15029300', 'Daily_Return': '-1.21', 'Price_Range': '9.25', 'Price_Change': '-4.04', 'MA_7': '410.78', 'MA_30': '', 'MA_90': '', 'Volatility_30d': '', 'Day_of_Week': 'Saturday', 'Month': '9', 'Year': '2014', 'Quarter': '3'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Full path to your CSV file\n",
    "file_path = \"/home/lenovo/Documents/FODS/Bitcoin_Price_Dataset_2014_2023.csv\"\n",
    "\n",
    "# Open and read the CSV file\n",
    "rows = []\n",
    "with open(file_path, 'r') as file:\n",
    "    reader = csv.DictReader(file)  # DictReader reads each row as a dictionary\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "print(f\"Total rows loaded: {len(rows)}\")\n",
    "\n",
    "# Print row 10 (index 10)\n",
    "print(\"Sample row:\", rows[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110858a",
   "metadata": {},
   "source": [
    "#Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7138af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 3364\n"
     ]
    }
   ],
   "source": [
    "# We'll use these columns as features (inputs) and Close as the target (output)\n",
    "features_to_use = ['Open', 'High', 'Low', 'Volume', 'MA_7', 'MA_30']\n",
    "target = 'Close'\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        # Try to convert each needed column to a float\n",
    "        entry = {}\n",
    "        for col in features_to_use + [target]:\n",
    "            val = row[col].strip()\n",
    "            if val == '' or val == 'None':\n",
    "                raise ValueError(\"Missing value\")  # Skip rows with empty values\n",
    "            entry[col] = float(val)\n",
    "        clean_data.append(entry)\n",
    "    except ValueError:\n",
    "        pass  # Skip this row if any value is missing or not a number\n",
    "\n",
    "print(f\"Rows after cleaning: {len(clean_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eba811",
   "metadata": {},
   "source": [
    "#Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af78143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 2691\n",
      "Testing rows: 673\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(clean_data)  # Shuffle so we don't train only on old data\n",
    "\n",
    "split_index = int(len(clean_data) * 0.8)  # 80% for training\n",
    "\n",
    "train_data = clean_data[:split_index]\n",
    "test_data = clean_data[split_index:]\n",
    "\n",
    "print(f\"Training rows: {len(train_data)}\")\n",
    "print(f\"Testing rows: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548a452",
   "metadata": {},
   "source": [
    "#Preparing Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f1d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 2691 rows, 6 features\n"
     ]
    }
   ],
   "source": [
    "def get_X_y(data, feature_cols, target_col):\n",
    "    \"\"\"Extract features (X) and target (y) from the data\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for row in data:\n",
    "        X.append([row[col] for col in feature_cols])  # List of feature values\n",
    "        y.append(row[target_col])                      # Target value\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = get_X_y(train_data, features_to_use, target)\n",
    "X_test, y_test = get_X_y(test_data, features_to_use, target)\n",
    "\n",
    "print(f\"X_train shape: {len(X_train)} rows, {len(X_train[0])} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c24c0",
   "metadata": {},
   "source": [
    "#Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1af542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Scale each feature to be between 0 and 1\"\"\"\n",
    "    num_features = len(X[0])\n",
    "    mins = [min(row[i] for row in X) for i in range(num_features)]\n",
    "    maxs = [max(row[i] for row in X) for i in range(num_features)]\n",
    "    \n",
    "    X_scaled = []\n",
    "    for row in X:\n",
    "        scaled_row = []\n",
    "        for i in range(num_features):\n",
    "            if maxs[i] - mins[i] == 0:\n",
    "                scaled_row.append(0)  # Avoid division by zero\n",
    "            else:\n",
    "                scaled_row.append((row[i] - mins[i]) / (maxs[i] - mins[i]))\n",
    "        X_scaled.append(scaled_row)\n",
    "    \n",
    "    return X_scaled, mins, maxs\n",
    "\n",
    "X_train_scaled, mins, maxs = normalize(X_train)\n",
    "# Use the SAME mins/maxs from training to scale test data\n",
    "X_test_scaled = [[(row[i] - mins[i]) / (maxs[i] - mins[i]) if maxs[i] - mins[i] != 0 else 0\n",
    "                  for i in range(len(row))] for row in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc363",
   "metadata": {},
   "source": [
    "#Implentation of Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed571404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 486110176.59\n",
      "Epoch 100: MSE = 113315963.27\n",
      "Epoch 200: MSE = 58117530.65\n",
      "Epoch 300: MSE = 35915745.86\n",
      "Epoch 400: MSE = 22821092.51\n",
      "Epoch 500: MSE = 14642575.86\n",
      "Epoch 600: MSE = 9500462.79\n",
      "Epoch 700: MSE = 6264909.34\n",
      "Epoch 800: MSE = 4228703.21\n",
      "Epoch 900: MSE = 2947118.38\n"
     ]
    }
   ],
   "source": [
    "def predict(X, weights, bias):\n",
    "    \"\"\"Make predictions: y = w1*x1 + w2*x2 + ... + bias\"\"\"\n",
    "    results = []\n",
    "    for row in X:\n",
    "        pred = bias\n",
    "        for i in range(len(row)):\n",
    "            pred += weights[i] * row[i]\n",
    "        results.append(pred)\n",
    "    return results\n",
    "\n",
    "def train_linear_regression(X, y, learning_rate=0.01, epochs=1000):\n",
    "    \"\"\"Train using gradient descent\"\"\"\n",
    "    n = len(X)                        # Number of training examples\n",
    "    num_features = len(X[0])\n",
    "    \n",
    "    # Start with all weights = 0\n",
    "    weights = [0.0] * num_features\n",
    "    bias = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Make predictions with current weights\n",
    "        predictions = predict(X, weights, bias)\n",
    "        \n",
    "        # Step 2: Calculate errors\n",
    "        errors = [predictions[i] - y[i] for i in range(n)]\n",
    "        \n",
    "        # Step 3: Calculate gradients (how much to adjust each weight)\n",
    "        weight_gradients = [0.0] * num_features\n",
    "        for i in range(n):\n",
    "            for j in range(num_features):\n",
    "                weight_gradients[j] += errors[i] * X[i][j]\n",
    "        \n",
    "        bias_gradient = sum(errors)\n",
    "        \n",
    "        # Step 4: Update weights (move in the direction that reduces error)\n",
    "        for j in range(num_features):\n",
    "            weights[j] -= learning_rate * (weight_gradients[j] / n)\n",
    "        bias -= learning_rate * (bias_gradient / n)\n",
    "        \n",
    "        # Print progress every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            mse = sum(e**2 for e in errors) / n\n",
    "            print(f\"Epoch {epoch}: MSE = {mse:.2f}\")\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "weights, bias = train_linear_regression(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a26c5",
   "metadata": {},
   "source": [
    "#Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccaf430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "Mean Squared Error: 2209456.21\n",
      "Root MSE: 1486.42\n",
      "R² Score: 0.9911\n",
      "\n",
      "Sample predictions vs actual:\n",
      "  Predicted: $8255.20  |  Actual: $6853.84\n",
      "  Predicted: $1541.16  |  Actual: $247.53\n",
      "  Predicted: $7411.75  |  Actual: $6529.59\n",
      "  Predicted: $20206.34  |  Actual: $21161.52\n",
      "  Predicted: $21939.95  |  Actual: $23389.43\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    return sum((y_true[i] - y_pred[i])**2 for i in range(n)) / n\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    \"\"\"R² tells us how well our model explains the variance in the data\"\"\"\n",
    "    mean_y = sum(y_true) / len(y_true)\n",
    "    ss_total = sum((y - mean_y)**2 for y in y_true)      # Total variance\n",
    "    ss_residual = sum((y_true[i] - y_pred[i])**2 for i in range(len(y_true)))  # Unexplained variance\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = predict(X_test_scaled, weights, bias)\n",
    "\n",
    "mse = mean_squared_error(y_test, test_predictions)\n",
    "r2 = r_squared(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root MSE: {mse**0.5:.2f}\")  # In the same units as price (dollars)\n",
    "print(f\"R² Score: {r2:.4f}\")        # Closer to 1.0 is better\n",
    "\n",
    "# Show a few example predictions vs actual\n",
    "print(\"\\nSample predictions vs actual:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Predicted: ${test_predictions[i]:.2f}  |  Actual: ${y_test[i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
